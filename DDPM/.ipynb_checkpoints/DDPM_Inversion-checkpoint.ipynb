{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f23450bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758b38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, dim, cond_dim):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.k = nn.Linear(dim + cond_dim, dim)\n",
    "        self.v = nn.Linear(dim + cond_dim, dim)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        B, C, H, W = x.shape\n",
    "        x_flat = x.view(B, C, -1).permute(0, 2, 1)  # (B, N, C)\n",
    "        cond = cond.unsqueeze(1).repeat(1, x_flat.size(1), 1)  # (B, N, cond_dim)\n",
    "\n",
    "        q = self.q(x_flat)\n",
    "        k = self.k(torch.cat([x_flat, cond], dim=-1))\n",
    "        v = self.v(torch.cat([x_flat, cond], dim=-1))\n",
    "\n",
    "        attn = torch.softmax(q @ k.transpose(-2, -1) / (C ** 0.5), dim=-1)\n",
    "        out = attn @ v\n",
    "        out = self.proj(out)\n",
    "        return out.permute(0, 2, 1).view(B, C, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0c68b7-254e-4263-ab27-09347763bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionUNet(nn.Module):\n",
    "    def __init__(self, cond_dim=128):\n",
    "        super().__init__()\n",
    "        self.cond_proj = nn.Linear(5 * 1000 * 70, cond_dim)\n",
    "\n",
    "        self.enc1 = nn.Conv2d(1, 64, 3, padding=1)\n",
    "        self.attn1 = AttentionBlock(64, cond_dim)\n",
    "        self.enc2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.attn2 = AttentionBlock(128, cond_dim)\n",
    "        self.bottleneck = nn.Conv2d(128, 256, 3, padding=1)\n",
    "\n",
    "        self.up1_conv = nn.Conv2d(256, 128, 3, padding=1)\n",
    "        self.up2_conv = nn.Conv2d(128, 64, 3, padding=1)\n",
    "        self.out = nn.Conv2d(64, 1, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t, cond):\n",
    "        B = cond.size(0)\n",
    "        cond = self.cond_proj(cond.view(B, -1)).float()\n",
    "\n",
    "        x1 = F.relu(self.enc1(x))        # [B, 64, 70, 70]\n",
    "        x1 = self.attn1(x1, cond)\n",
    "\n",
    "        x2 = F.avg_pool2d(x1, 2)         # [B, 64, 35, 35]\n",
    "        x2 = F.relu(self.enc2(x2))       # [B, 128, 35, 35]\n",
    "        x2 = self.attn2(x2, cond)\n",
    "\n",
    "        x3 = F.avg_pool2d(x2, 2)         # [B, 128, 17, 17]\n",
    "        x3 = F.relu(self.bottleneck(x3)) # [B, 256, 17, 17]\n",
    "\n",
    "        u1 = F.interpolate(x3, size=x2.shape[2:], mode='bilinear', align_corners=False)\n",
    "        u1 = F.relu(self.up1_conv(u1)) + x2\n",
    "\n",
    "        u2 = F.interpolate(u1, size=x1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        u2 = F.relu(self.up2_conv(u2)) + x1\n",
    "\n",
    "        return self.out(u2)  # → [B, 1, 70, 70]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de63b5b4-f9fe-488b-a808-aed99a1d74f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 1e-2\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps)\n",
    "\n",
    "def get_alphas(betas):\n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "    sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "    sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)\n",
    "    return sqrt_alphas_cumprod, sqrt_one_minus_alphas_cumprod\n",
    "\n",
    "def q_sample(x_start, t, noise, sqrt_alpha_cum, sqrt_1m_alpha_cum):\n",
    "    return (\n",
    "        sqrt_alpha_cum[t][:, None, None, None] * x_start +\n",
    "        sqrt_1m_alpha_cum[t][:, None, None, None] * noise\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8252fa5a-282c-485e-a1a0-e24b9e79a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class OpenFWIDataset(Dataset):\n",
    "    def __init__(self, data_dir, split=\"train\"):\n",
    "        assert split in [\"train\", \"val\"]\n",
    "        self.waves = np.load(os.path.join(data_dir, f\"{split}_waves.npy\"))  # e.g., (N, C, T, W)\n",
    "        self.vels = np.load(os.path.join(data_dir, f\"{split}_vels.npy\"))    # e.g., (N, 1, H, W)\n",
    "\n",
    "        # normalize (optional)\n",
    "        self.waves = self.waves.astype(np.float32)\n",
    "        self.vels = self.vels.astype(np.float32)\n",
    "        self.waves /= 60.0\n",
    "        self.vels /= 4500.0\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.waves)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wave = torch.from_numpy(self.waves[idx])  # shape: (C, T, W)\n",
    "        vel  = torch.from_numpy(self.vels[idx])   # shape: (1, H, W)\n",
    "        return wave, vel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96569b30-07ee-4fed-83e2-10aa151c3725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wave: torch.Size([8, 5, 1000, 70]), vel: torch.Size([8, 1, 70, 70])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = OpenFWIDataset(\"../dataset_one_batch\", split=\"train\")\n",
    "val_set   = OpenFWIDataset(\"../dataset_one_batch\", split=\"val\")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_set, batch_size=8, shuffle=False)\n",
    "\n",
    "# 1バッチ確認\n",
    "for xb, yb in train_loader:\n",
    "    print(f\"wave: {xb.shape}, vel: {yb.shape}\")  # 例: (8, 5, 1000, 70), (8, 1, 70, 70)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58ca0f37-4838-4c50-95bf-ed1213050e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 訓練設定 ---\n",
    "T = 1000\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 8\n",
    "SAVE_PATH = \"ddpm_model2.pt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- 拡散スケジュール ---\n",
    "betas = linear_beta_schedule(T).to(device)\n",
    "sqrt_alpha_cum, sqrt_1m_alpha_cum = get_alphas(betas)\n",
    "\n",
    "# --- データ / モデル ---\n",
    "train_ds = OpenFWIDataset(\"../dataset_one_batch\", split=\"train\")\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96bccfbb-3b6a-49fe-89d8-d38893434e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionUNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cfe6cd67-0041-4ee1-b5d1-fd5011c42d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 01] Train MSE: 0.9998  Val MSE: 4848.5299  Val MAE: 900701.8325   <-- best so far\n",
      "[Epoch 02] Train MSE: 0.8306  Val MSE: 1749.3080  Val MAE: 421651.6265   <-- best so far\n",
      "[Epoch 03] Train MSE: 0.2630  Val MSE: 1056.9027  Val MAE: 301549.6487   <-- best so far\n",
      "[Epoch 04] Train MSE: 0.1340  Val MSE: 446.6248  Val MAE: 134014.5612   <-- best so far\n",
      "[Epoch 05] Train MSE: 0.0871  Val MSE: 310.5471  Val MAE: 96886.7266   <-- best so far\n",
      "[Epoch 06] Train MSE: 0.0749  Val MSE: 300.2826  Val MAE: 107455.8274   <-- best so far\n",
      "[Epoch 07] Train MSE: 0.0604  Val MSE: 259.7316  Val MAE: 110581.9413   <-- best so far\n",
      "[Epoch 08] Train MSE: 0.0591  Val MSE: 409.6258  Val MAE: 89791.4080 \n",
      "[Epoch 09] Train MSE: 0.0480  Val MSE: 192.8938  Val MAE: 70977.1182   <-- best so far\n",
      "[Epoch 10] Train MSE: 0.0510  Val MSE: 231.7322  Val MAE: 67900.0591 \n",
      "[Epoch 11] Train MSE: 0.0489  Val MSE: 172.1675  Val MAE: 84148.8545   <-- best so far\n",
      "[Epoch 12] Train MSE: 0.0468  Val MSE: 198.1336  Val MAE: 66431.6303 \n",
      "[Epoch 13] Train MSE: 0.0449  Val MSE: 209.4120  Val MAE: 59976.2049 \n",
      "[Epoch 14] Train MSE: 0.0332  Val MSE: 168.6186  Val MAE: 64794.7202   <-- best so far\n",
      "[Epoch 15] Train MSE: 0.0349  Val MSE: 176.5896  Val MAE: 74824.9471 \n",
      "[Epoch 16] Train MSE: 0.0344  Val MSE: 167.9093  Val MAE: 64184.7316   <-- best so far\n",
      "[Epoch 17] Train MSE: 0.0337  Val MSE: 141.3916  Val MAE: 61323.8203   <-- best so far\n",
      "[Epoch 18] Train MSE: 0.0364  Val MSE: 121.9497  Val MAE: 52763.7840   <-- best so far\n",
      "[Epoch 19] Train MSE: 0.0341  Val MSE: 244.7756  Val MAE: 78317.6590 \n",
      "[Epoch 20] Train MSE: 0.0425  Val MSE: 153.3542  Val MAE: 83952.6782 \n",
      "[Epoch 21] Train MSE: 0.0325  Val MSE: 226.8040  Val MAE: 70090.3857 \n",
      "[Epoch 22] Train MSE: 0.0222  Val MSE: 154.1023  Val MAE: 71103.9095 \n",
      "[Epoch 23] Train MSE: 0.0241  Val MSE: 157.7767  Val MAE: 66012.9480 \n",
      "[Epoch 24] Train MSE: 0.0251  Val MSE: 162.7000  Val MAE: 72156.5233 \n",
      "[Epoch 25] Train MSE: 0.0260  Val MSE: 199.7973  Val MAE: 47824.2629 \n",
      "[Epoch 26] Train MSE: 0.0261  Val MSE: 128.0465  Val MAE: 66135.6552 \n",
      "[Epoch 27] Train MSE: 0.0265  Val MSE: 108.1377  Val MAE: 64784.9046   <-- best so far\n",
      "[Epoch 28] Train MSE: 0.0270  Val MSE: 129.9783  Val MAE: 62758.4879 \n",
      "[Epoch 29] Train MSE: 0.0302  Val MSE: 152.3803  Val MAE: 60666.8736 \n",
      "[Epoch 30] Train MSE: 0.0269  Val MSE: 138.5872  Val MAE: 58994.4397 \n",
      "[Epoch 31] Train MSE: 0.0305  Val MSE: 199.5666  Val MAE: 74407.8295 \n",
      "[Epoch 32] Train MSE: 0.0288  Val MSE: 155.9366  Val MAE: 72699.2961 \n",
      "[Epoch 33] Train MSE: 0.0278  Val MSE: 104.8126  Val MAE: 61767.6779   <-- best so far\n",
      "[Epoch 34] Train MSE: 0.0276  Val MSE: 124.6530  Val MAE: 60572.9882 \n",
      "[Epoch 35] Train MSE: 0.0272  Val MSE: 114.6142  Val MAE: 63191.6624 \n",
      "[Epoch 36] Train MSE: 0.0275  Val MSE: 117.5410  Val MAE: 64353.2286 \n",
      "[Epoch 37] Train MSE: 0.0260  Val MSE: 107.0403  Val MAE: 80394.7811 \n",
      "[Epoch 38] Train MSE: 0.0229  Val MSE: 107.0586  Val MAE: 65660.7193 \n",
      "[Epoch 39] Train MSE: 0.0229  Val MSE: 110.3880  Val MAE: 49379.6469 \n",
      "[Epoch 40] Train MSE: 0.0265  Val MSE: 160.1808  Val MAE: 47648.5996 \n",
      "[Epoch 41] Train MSE: 0.0225  Val MSE: 104.6036  Val MAE: 62372.2291   <-- best so far\n",
      "[Epoch 42] Train MSE: 0.0277  Val MSE: 112.6809  Val MAE: 65540.0411 \n",
      "[Epoch 43] Train MSE: 0.0233  Val MSE: 137.7782  Val MAE: 62400.5281 \n",
      "[Epoch 44] Train MSE: 0.0240  Val MSE: 96.4094  Val MAE: 56154.7247   <-- best so far\n",
      "[Epoch 45] Train MSE: 0.0259  Val MSE: 97.2119  Val MAE: 45022.8228 \n",
      "[Epoch 46] Train MSE: 0.0237  Val MSE: 120.1564  Val MAE: 50893.3742 \n",
      "[Epoch 47] Train MSE: 0.0245  Val MSE: 116.3323  Val MAE: 52634.3024 \n",
      "[Epoch 48] Train MSE: 0.0249  Val MSE: 78.8377  Val MAE: 63172.1162   <-- best so far\n",
      "[Epoch 49] Train MSE: 0.0258  Val MSE: 104.2821  Val MAE: 65947.9075 \n",
      "[Epoch 50] Train MSE: 0.0246  Val MSE: 106.4455  Val MAE: 52457.3897 \n",
      "[Epoch 51] Train MSE: 0.0262  Val MSE: 88.9011  Val MAE: 44032.6536 \n",
      "[Epoch 52] Train MSE: 0.0227  Val MSE: 110.5613  Val MAE: 60657.7108 \n",
      "[Epoch 53] Train MSE: 0.0202  Val MSE: 77.3418  Val MAE: 58471.9596   <-- best so far\n",
      "[Epoch 54] Train MSE: 0.0261  Val MSE: 92.2969  Val MAE: 58879.7201 \n",
      "[Epoch 55] Train MSE: 0.0205  Val MSE: 74.5807  Val MAE: 59204.9088   <-- best so far\n",
      "[Epoch 56] Train MSE: 0.0257  Val MSE: 88.0846  Val MAE: 53923.5607 \n",
      "[Epoch 57] Train MSE: 0.0238  Val MSE: 137.0946  Val MAE: 69763.5166 \n",
      "[Epoch 58] Train MSE: 0.0169  Val MSE: 83.6745  Val MAE: 51061.1704 \n",
      "[Epoch 59] Train MSE: 0.0211  Val MSE: 74.1406  Val MAE: 50913.6370   <-- best so far\n",
      "[Epoch 60] Train MSE: 0.0274  Val MSE: 98.0835  Val MAE: 51977.9323 \n",
      "[Epoch 61] Train MSE: 0.0198  Val MSE: 98.9075  Val MAE: 55532.5858 \n",
      "[Epoch 62] Train MSE: 0.0179  Val MSE: 110.6700  Val MAE: 59055.5669 \n",
      "[Epoch 63] Train MSE: 0.0221  Val MSE: 117.6327  Val MAE: 63126.7930 \n",
      "[Epoch 64] Train MSE: 0.0229  Val MSE: 67.3470  Val MAE: 58374.2881   <-- best so far\n",
      "[Epoch 65] Train MSE: 0.0216  Val MSE: 102.2140  Val MAE: 48848.4930 \n",
      "[Epoch 66] Train MSE: 0.0236  Val MSE: 70.9419  Val MAE: 63556.0730 \n",
      "[Epoch 67] Train MSE: 0.0230  Val MSE: 101.5251  Val MAE: 58459.5733 \n",
      "[Epoch 68] Train MSE: 0.0178  Val MSE: 102.0317  Val MAE: 56006.6559 \n",
      "[Epoch 69] Train MSE: 0.0215  Val MSE: 89.9398  Val MAE: 58637.9900 \n",
      "[Epoch 70] Train MSE: 0.0179  Val MSE: 106.5131  Val MAE: 64266.2852 \n",
      "[Epoch 71] Train MSE: 0.0198  Val MSE: 89.7928  Val MAE: 71345.7753 \n",
      "[Epoch 72] Train MSE: 0.0213  Val MSE: 78.0338  Val MAE: 51740.3034 \n",
      "[Epoch 73] Train MSE: 0.0137  Val MSE: 97.6071  Val MAE: 50562.2673 \n",
      "[Epoch 74] Train MSE: 0.0179  Val MSE: 84.9306  Val MAE: 49075.2989 \n",
      "[Epoch 75] Train MSE: 0.0162  Val MSE: 97.4528  Val MAE: 51411.6195 \n",
      "[Epoch 76] Train MSE: 0.0225  Val MSE: 77.0474  Val MAE: 72227.6131 \n",
      "[Epoch 77] Train MSE: 0.0203  Val MSE: 153.5191  Val MAE: 44975.2781 \n",
      "[Epoch 78] Train MSE: 0.0190  Val MSE: 99.1308  Val MAE: 44509.2294 \n",
      "[Epoch 79] Train MSE: 0.0199  Val MSE: 115.5535  Val MAE: 52964.0775 \n",
      "[Epoch 80] Train MSE: 0.0175  Val MSE: 146.8893  Val MAE: 52743.4418 \n",
      "[Epoch 81] Train MSE: 0.0138  Val MSE: 107.9159  Val MAE: 46109.7971 \n",
      "[Epoch 82] Train MSE: 0.0206  Val MSE: 64.8037  Val MAE: 50267.4471   <-- best so far\n",
      "[Epoch 83] Train MSE: 0.0225  Val MSE: 158.6092  Val MAE: 58865.8148 \n",
      "[Epoch 84] Train MSE: 0.0176  Val MSE: 103.5677  Val MAE: 56247.8012 \n",
      "[Epoch 85] Train MSE: 0.0218  Val MSE: 88.2997  Val MAE: 53330.6183 \n",
      "[Epoch 86] Train MSE: 0.0200  Val MSE: 94.8848  Val MAE: 54170.1167 \n",
      "[Epoch 87] Train MSE: 0.0182  Val MSE: 95.2920  Val MAE: 50402.1862 \n",
      "[Epoch 88] Train MSE: 0.0165  Val MSE: 91.2517  Val MAE: 53149.2233 \n",
      "[Epoch 89] Train MSE: 0.0157  Val MSE: 103.5000  Val MAE: 48893.5519 \n",
      "[Epoch 90] Train MSE: 0.0181  Val MSE: 88.7442  Val MAE: 49987.3258 \n",
      "[Epoch 91] Train MSE: 0.0180  Val MSE: 82.7339  Val MAE: 39813.5408 \n",
      "[Epoch 92] Train MSE: 0.0192  Val MSE: 132.1330  Val MAE: 51644.9606 \n",
      "[Epoch 93] Train MSE: 0.0198  Val MSE: 58.1335  Val MAE: 61855.1921   <-- best so far\n",
      "[Epoch 94] Train MSE: 0.0255  Val MSE: 119.0730  Val MAE: 63761.6465 \n",
      "[Epoch 95] Train MSE: 0.0156  Val MSE: 103.0554  Val MAE: 55605.8654 \n",
      "[Epoch 96] Train MSE: 0.0209  Val MSE: 116.7319  Val MAE: 49420.9131 \n",
      "[Epoch 97] Train MSE: 0.0193  Val MSE: 87.2322  Val MAE: 48705.2921 \n",
      "[Epoch 98] Train MSE: 0.0182  Val MSE: 59.2005  Val MAE: 49380.3320 \n",
      "[Epoch 99] Train MSE: 0.0182  Val MSE: 81.4136  Val MAE: 62090.5189 \n",
      "[Epoch 100] Train MSE: 0.0212  Val MSE: 105.9190  Val MAE: 41083.1997 \n",
      "\n",
      "Training complete. Best validation MSE: 58.1335\n",
      "Best model saved to: best_ddpm_model.pt\n"
     ]
    }
   ],
   "source": [
    "# train_ddpm_best.py\n",
    "\n",
    "\n",
    "\n",
    "def train():\n",
    "    # 1. 設定\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    T = 1000\n",
    "    EPOCHS = 100\n",
    "    BATCH_SIZE = 8\n",
    "    SAVE_PATH = \"best_ddpm_model.pt\"\n",
    "\n",
    "    # 2. 拡散スケジュール\n",
    "    betas = linear_beta_schedule(T).to(device)\n",
    "    sqrt_alpha_cum, sqrt_1m_alpha_cum = get_alphas(betas)\n",
    "\n",
    "    # 3. データローダー\n",
    "    train_ds = OpenFWIDataset(\"../dataset_one_batch\", split=\"train\")\n",
    "    val_ds   = OpenFWIDataset(\"../dataset_one_batch\", split=\"val\")\n",
    "    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # 4. モデル＆最適化\n",
    "    model = AttentionUNet().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=5, factor=0.5\n",
    "    )\n",
    "    best_val_loss = float(\"inf\")\n",
    "\n",
    "    # 5. 学習ループ\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        total_train = 0.0\n",
    "        for seismic, velocity in train_dl:\n",
    "            seismic, velocity = seismic.to(device), velocity.to(device)\n",
    "            B = velocity.size(0)\n",
    "\n",
    "            # ランダムタイムステップ＆ノイズサンプリング\n",
    "            t = torch.randint(0, T, (B,), device=device).long()\n",
    "            noise = torch.randn_like(velocity)\n",
    "            x_t = sqrt_alpha_cum[t][:, None, None, None] * velocity \\\n",
    "                + sqrt_1m_alpha_cum[t][:, None, None, None] * noise\n",
    "\n",
    "            # 条件flatten\n",
    "            cond = seismic.view(B, -1)\n",
    "\n",
    "            # 予測＆損失計算（MSE）\n",
    "            pred_noise = model(x_t, t, cond)\n",
    "            loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "            # 逆伝播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_train += loss.item() * B\n",
    "\n",
    "        avg_train = total_train / len(train_ds)\n",
    "\n",
    "        # --- validation ---\n",
    "        model.eval()\n",
    "        total_val_mse = 0.0\n",
    "        total_val_mae = 0.0\n",
    "        with torch.no_grad():\n",
    "            for seismic, velocity in val_dl:\n",
    "                seismic, velocity = seismic.to(device), velocity.to(device)\n",
    "                B = velocity.size(0)\n",
    "\n",
    "                # 同様のノイズ注入\n",
    "                t = torch.randint(0, T, (B,), device=device).long()\n",
    "                noise = torch.randn_like(velocity)\n",
    "                x_t = sqrt_alpha_cum[t][:, None, None, None] * velocity \\\n",
    "                    + sqrt_1m_alpha_cum[t][:, None, None, None] * noise\n",
    "\n",
    "                cond = seismic.view(B, -1)\n",
    "                pred_noise = model(x_t, t, cond)\n",
    "\n",
    "                # Val MSE on noise\n",
    "                total_val_mse += F.mse_loss(pred_noise, noise, reduction=\"sum\").item()\n",
    "\n",
    "                # Val MAE on reconstructed velocity\n",
    "                pred_vel = (x_t - sqrt_1m_alpha_cum[t][:, None, None, None] * pred_noise) \\\n",
    "                           / sqrt_alpha_cum[t][:, None, None, None]\n",
    "                total_val_mae += F.l1_loss(pred_vel, velocity, reduction=\"sum\").item()\n",
    "\n",
    "        avg_val_mse = total_val_mse / len(val_ds)\n",
    "        avg_val_mae = total_val_mae / len(val_ds)\n",
    "\n",
    "        # --- ベストモデル更新（MSE基準）---\n",
    "        improved = \"\"\n",
    "        if avg_val_mse < best_val_loss:\n",
    "            best_val_loss = avg_val_mse\n",
    "            torch.save(model.state_dict(), SAVE_PATH)\n",
    "            improved = \"  <-- best so far\"\n",
    "\n",
    "        # LRスケジューラ更新\n",
    "        scheduler.step(avg_val_mse)\n",
    "\n",
    "        # ログ出力\n",
    "        print(\n",
    "            f\"[Epoch {epoch:02d}] \"\n",
    "            f\"Train MSE: {avg_train:.4f}  \"\n",
    "            f\"Val MSE: {avg_val_mse:.4f}  \"\n",
    "            f\"Val MAE: {avg_val_mae:.4f} {improved}\"\n",
    "        )\n",
    "\n",
    "    print(f\"\\nTraining complete. Best validation MSE: {best_val_loss:.4f}\")\n",
    "    print(f\"Best model saved to: {SAVE_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbd5c2e2-8275-404f-b2c8-d5bd97d59eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "# 0. GPU設定\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1. データ＆モデル読み込み\n",
    "val_ds = OpenFWIDataset(\"../dataset_one_batch\", split=\"val\")\n",
    "wave, gt_vel = val_ds[0]                    # wave: [C,T,W], gt_vel: [1,H,W]\n",
    "wave = wave.unsqueeze(0).to(device)         # [1,C,T,W]\n",
    "cond = wave.view(1, -1)                     # [1, C*T*W]\n",
    "gt_vel = gt_vel.unsqueeze(0).to(device)     # [1,1,H,W]\n",
    "\n",
    "model = AttentionUNet().to(device)\n",
    "model.load_state_dict(torch.load(\"best_ddpm_model.pt\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 2. 拡散スケジュール\n",
    "T = 1000\n",
    "betas = linear_beta_schedule(T).to(device)    # β₁…β_T\n",
    "alphas = 1 - betas                           # α₁…α_T\n",
    "alpha_cum = torch.cumprod(alphas, dim=0)      # ᾱ_t\n",
    "sqrt_alpha_cum = alpha_cum.sqrt()             \n",
    "sqrt_1m_alpha_cum = (1 - alpha_cum).sqrt()\n",
    "\n",
    "# 3. 逆拡散サンプリング関数\n",
    "@torch.no_grad()\n",
    "def p_sample(x, t, cond):\n",
    "    \"\"\"\n",
    "    x: [B,1,H,W]            ノイズ付加済みサンプル x_t\n",
    "    t: [B]                  タイムステップ\n",
    "    cond: [B, C*T*W]        flattenされた seismic\n",
    "    \"\"\"\n",
    "    eps_pred = model(x, t, cond)           # ノイズ予測\n",
    "    beta_t = betas[t]\n",
    "    alpha_t = alphas[t]\n",
    "    abar_t = alpha_cum[t]\n",
    "    # abar_prev は t=0 のとき1.0\n",
    "    abar_prev = torch.cat([torch.tensor([1.0], device=device), alpha_cum[:-1]])[t]\n",
    "\n",
    "    # 平均 μ_t(x_t)\n",
    "    coef = beta_t / sqrt_1m_alpha_cum[t]\n",
    "    mean = (x - coef.view(-1,1,1,1) * eps_pred) / torch.sqrt(alpha_t).view(-1,1,1,1)\n",
    "\n",
    "    # 分散 β̂_t = β_t * (1−ᾱ_{t−1})/(1−ᾱ_t)\n",
    "    var = beta_t * (1 - abar_prev) / (1 - abar_t)\n",
    "    if (t > 0).any():\n",
    "        noise = torch.randn_like(x)\n",
    "        return mean + torch.sqrt(var).view(-1,1,1,1) * noise\n",
    "    else:\n",
    "        return mean\n",
    "\n",
    "# 4. 全ステップ逆拡散ループ\n",
    "x = torch.randn_like(gt_vel)  # x_T をノイズで初期化\n",
    "for ti in trange(T-1, -1, -1, desc=\"Reverse Sampling\"):\n",
    "    t = torch.full((1,), ti, dtype=torch.long, device=device)\n",
    "    x = p_sample(x, t, cond)\n",
    "\n",
    "# 5. 可視化（[0,1]→元スケールへ）\n",
    "pred = x.clamp(0,1).squeeze().cpu().numpy() * 4500\n",
    "gt   = gt_vel.squeeze().cpu().numpy() * 4500\n",
    "\n",
    "fig, axs = plt.subplots(1,2,figsize=(10,4))\n",
    "axs[0].imshow(gt,   cmap=\"viridis\"); axs[0].set_title(\"Ground Truth\")\n",
    "axs[1].imshow(pred, cmap=\"viridis\"); axs[1].set_title(\"DDPM Full Sampling\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe6efe9-2714-47b6-9541-b23d205def42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
