{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ セットアップ（ライブラリインポート）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ モデル定義：InverseVelocityDecoder（波形→速度マップ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InverseVelocityDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(1, 16, (3, 5, 3), padding=1)\n",
    "        self.conv2 = nn.Conv3d(16, 32, 3, padding=1)\n",
    "        self.pool = nn.AdaptiveAvgPool3d((1, 10, 10))\n",
    "        self.fc = nn.Linear(32 * 10 * 10, 1024)\n",
    "        self.out = nn.Linear(1024, 70 * 70)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        return self.out(x).view(-1, 1, 70, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ モデル定義：WaveNet（Forward PINNでu(x,z,t)を出力）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNet(nn.Module):\n",
    "    def __init__(self, in_dim=3, hidden=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.Tanh(),\n",
    "            nn.Linear(hidden, hidden), nn.Tanh(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, xzt):\n",
    "        return self.net(xzt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ VelocityMapFn（速度マップを連続関数化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VelocityMapFn(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, waveform, coords):\n",
    "        v_grid = self.model(waveform)  # [1, 1, 70, 70]\n",
    "        \n",
    "        # 座標数だけv_gridを複製： [512, 1, 70, 70]\n",
    "        v_grid = v_grid.expand(coords.shape[0], -1, -1, -1)\n",
    "    \n",
    "        coords = (coords + 1) / 2  # [-1,1] → [0,1]\n",
    "        coords = coords.unsqueeze(1).unsqueeze(1)  # → [512, 1, 1, 2]\n",
    "        \n",
    "        return F.grid_sample(v_grid, coords, align_corners=True).view(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ PDE残差計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pde_residual(u_model, v_fn, waveform, xzt):\n",
    "    xzt = xzt.clone().detach().requires_grad_(True)  # ✅ これで確実に autograd が働く\n",
    "    u = u_model(xzt)\n",
    "    grads = torch.autograd.grad(u, xzt, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "    u_x, u_z, u_t = grads[:, 0:1], grads[:, 1:2], grads[:, 2:3]\n",
    "    u_xx = torch.autograd.grad(u_x, xzt, grad_outputs=torch.ones_like(u_x), create_graph=True)[0][:, 0:1]\n",
    "    u_zz = torch.autograd.grad(u_z, xzt, grad_outputs=torch.ones_like(u_z), create_graph=True)[0][:, 1:2]\n",
    "    u_tt = torch.autograd.grad(u_t, xzt, grad_outputs=torch.ones_like(u_t), create_graph=True)[0][:, 2:3]\n",
    "    v = v_fn(waveform, xzt[:, :2])\n",
    "    residual = u_tt - v**2 * (u_xx + u_zz)\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 座標サンプラー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_coords(batch_size):\n",
    "    return 2.0 * torch.rand(batch_size, 3) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 2) Dataset 準備\n",
    "# ----------------------------\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class WaveformDataset(Dataset):\n",
    "    def __init__(self, waves_path, vels_path):\n",
    "        self.waves = np.load(waves_path)  # shape: [N, 5, 1000, 7]\n",
    "        self.vels  = np.load(vels_path)   # shape: [N, 1, 70, 70]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.waves)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.waves[idx]).float()\n",
    "        y = torch.from_numpy(self.vels[idx]).float()\n",
    "        return x, y\n",
    "\n",
    "# パスにある .npy データをロード\n",
    "train_ds = WaveformDataset(\"./dataset_one_batch/train_waves.npy\",\n",
    "                           \"./dataset_one_batch/train_vels.npy\")\n",
    "val_ds   = WaveformDataset(\"./dataset_one_batch/val_waves.npy\",\n",
    "                           \"./dataset_one_batch/val_vels.npy\")\n",
    "\n",
    "# DataLoaderの準備\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=16, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader size: 100\n",
      "Batch x shape: torch.Size([16, 5, 1000, 70])\n"
     ]
    }
   ],
   "source": [
    "print(\"train_loader size:\", len(train_loader))\n",
    "for x, y in train_loader:\n",
    "    print(\"Batch x shape:\", x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 学習 + 検証ループ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forward_pinn_with_val(\n",
    "    inverse_cnn, velocity_map_fn, wave_model,\n",
    "    train_loader, val_loader, optimizer, device,\n",
    "    epochs=200, best_model_path=\"best_forward_pinn_model.pth\"):\n",
    "    best_loss = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in range(epochs):\n",
    "        inverse_cnn.train(); wave_model.train()\n",
    "        total_loss = 0.0\n",
    "        for xb, _ in train_loader:\n",
    "            xb = xb.to(device)\n",
    "        \n",
    "            batch_loss = 0.0\n",
    "            for i in range(xb.size(0)):  # バッチサイズ分ループ（例：16回）\n",
    "                xzt = sample_coords(batch_size=512).to(device)\n",
    "                xi = xb[i].unsqueeze(0)  # [1, 5, 1000, 70]\n",
    "                residual = compute_pde_residual(wave_model, velocity_map_fn, xi, xzt)\n",
    "                loss_pde = (residual ** 2).mean()\n",
    "                batch_loss += loss_pde\n",
    "        \n",
    "            batch_loss /= xb.size(0)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += batch_loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        # --- 検証 ---\n",
    "        wave_model.eval(); val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, _ in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                xzt = sample_coords(batch_size=512).to(device)\n",
    "                residual = compute_pde_residual(wave_model, velocity_map_fn, xb, xzt)\n",
    "                val_loss += (residual ** 2).mean().item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train: {avg_train_loss:.6f} | Val: {avg_val_loss:.6f}\")\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(inverse_cnn.state_dict(), best_model_path)\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:181.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(inverse_cnn\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 学習の実行\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m train_losses, val_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_forward_pinn_with_val\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43minverse_cnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvelocity_map_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwave_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbest_forward_pinn_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 36\u001b[0m, in \u001b[0;36mtrain_forward_pinn_with_val\u001b[0;34m(inverse_cnn, velocity_map_fn, wave_model, train_loader, val_loader, optimizer, device, epochs, best_model_path)\u001b[0m\n\u001b[1;32m     34\u001b[0m         xb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m         xzt \u001b[38;5;241m=\u001b[39m sample_coords(batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 36\u001b[0m         residual \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_pde_residual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwave_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvelocity_map_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxzt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m         val_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (residual \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     39\u001b[0m avg_val_loss \u001b[38;5;241m=\u001b[39m val_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(val_loader)\n",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m, in \u001b[0;36mcompute_pde_residual\u001b[0;34m(u_model, v_fn, waveform, xzt)\u001b[0m\n\u001b[1;32m      2\u001b[0m xzt \u001b[38;5;241m=\u001b[39m xzt\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# ✅ これで確実に autograd が働く\u001b[39;00m\n\u001b[1;32m      3\u001b[0m u \u001b[38;5;241m=\u001b[39m u_model(xzt)\n\u001b[0;32m----> 4\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxzt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m u_x, u_z, u_t \u001b[38;5;241m=\u001b[39m grads[:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m], grads[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m2\u001b[39m], grads[:, \u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m      6\u001b[0m u_xx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(u_x, xzt, grad_outputs\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mones_like(u_x), create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m][:, \u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:502\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    498\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    499\u001b[0m         grad_outputs_\n\u001b[1;32m    500\u001b[0m     )\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 502\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m    513\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[1;32m    515\u001b[0m     ):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# 3) Training 実行\n",
    "# ----------------------------\n",
    "\n",
    "# デバイスの指定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# モデルの初期化\n",
    "inverse_cnn = InverseVelocityDecoder().to(device)\n",
    "wave_model = WaveNet().to(device)\n",
    "velocity_map_fn = VelocityMapFn(inverse_cnn).to(device)\n",
    "\n",
    "# 最適化アルゴリズム\n",
    "optimizer = torch.optim.Adam(inverse_cnn.parameters(), lr=1e-3)\n",
    "\n",
    "# 学習の実行\n",
    "train_losses, val_losses = train_forward_pinn_with_val(\n",
    "    inverse_cnn,\n",
    "    velocity_map_fn,\n",
    "    wave_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    epochs=200,\n",
    "    best_model_path=\"best_forward_pinn_model.pth\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 可視化（Loss曲線）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(train_losses, val_losses):\n",
    "    plt.figure()\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('PDE Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training vs Validation Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 推論結果の可視化（予測 vs Ground Truth）\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 推論モードに切り替え\n",
    "inverse_cnn.eval()\n",
    "\n",
    "n_show = 5  # 表示するサンプル数\n",
    "\n",
    "with torch.no_grad():\n",
    "    xb, yb = next(iter(val_loader))\n",
    "    xb = xb.to(device)\n",
    "    yb = yb.to(device)\n",
    "    v_pred = inverse_cnn(xb)  # [B,1,70,70]\n",
    "\n",
    "# 描画\n",
    "fig, axes = plt.subplots(n_show, 2, figsize=(8, n_show * 3))\n",
    "\n",
    "for i in range(n_show):\n",
    "    pred_img = v_pred[i, 0].detach().cpu().numpy()\n",
    "    gt_img   = yb[i, 0].detach().cpu().numpy()\n",
    "\n",
    "    ax_gt = axes[i, 0]\n",
    "    im_gt = ax_gt.imshow(gt_img, cmap=\"jet\", aspect='auto')\n",
    "    ax_gt.set_title(f\"Ground Truth #{i}\")\n",
    "    fig.colorbar(im_gt, ax=ax_gt, fraction=0.046, pad=0.04)\n",
    "    ax_gt.axis(\"off\")\n",
    "\n",
    "    ax_pred = axes[i, 1]\n",
    "    im_pred = ax_pred.imshow(pred_img, cmap=\"jet\", aspect='auto')\n",
    "    ax_pred.set_title(f\"Prediction #{i}\")\n",
    "    fig.colorbar(im_pred, ax=ax_pred, fraction=0.046, pad=0.04)\n",
    "    ax_pred.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
