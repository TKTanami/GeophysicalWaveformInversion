{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99e087ba-384b-45be-982f-5c7a9994d301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b25b26-6f08-47ad-991b-ddd37318476a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints', 'CurveFault_A', 'CurveFault_B', 'CurveVel_A', 'CurveVel_B', 'FlatFault_A', 'FlatFault_B', 'FlatVel_A', 'FlatVel_B', 'Style_A', 'Style_B']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "base_dir = \"../models\"  # Kaggle環境における入力データのベースパス\n",
    "print(os.listdir(base_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dbbd0fd-a78a-43e1-96da-daec9a7d2aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models:\n",
      "  .ipynb_checkpoints:\n",
      "  CurveFault_A:\n",
      "    └ curvefault_a_l1_480.pth\n",
      "    └ curvefault_a_l2_480.pth\n",
      "  CurveFault_B:\n",
      "    └ curvefault_b_l1_480.pth\n",
      "    └ curvefault_b_l2_480.pth\n",
      "  CurveVel_A:\n",
      "    └ curvevel_a_l1_480.pth\n",
      "    └ curvevel_a_l2_480.pth\n",
      "  CurveVel_B:\n",
      "    └ curvevel_b_l1_480.pth\n",
      "    └ curvevel_b_l2_480.pth\n",
      "  FlatFault_A:\n",
      "    └ flatfault_a_l1_480.pth\n",
      "    └ flatfault_a_l2_480.pth\n",
      "  FlatFault_B:\n",
      "    └ flatfault_b_l1_480.pth\n",
      "    └ flatfault_b_l2_480.pth\n",
      "  FlatVel_A:\n",
      "    └ flatvel_a_l1_480.pth\n",
      "    └ flatvel_a_l2_480.pth\n",
      "  FlatVel_B:\n",
      "    └ flatvel_b_l1_480.pth\n",
      "    └ flatvel_b_l2_480.pth\n",
      "  Style_A:\n",
      "    └ style_a_l1_480.pth\n",
      "    └ style_a_l2_480.pth\n",
      "  Style_B:\n",
      "    └ style_b_l1_480.pth\n",
      "    └ style_b_l2_480.pth\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(data_dir):\n",
    "    # ルートディレクトリ名をインデント付きで表示\n",
    "    depth = root.replace(data_dir, \"\").count(os.sep)\n",
    "    indent = \"  \" * depth\n",
    "    print(f\"{indent}{os.path.basename(root) or data_dir}:\")\n",
    "    # すべての .npy ファイルを表示\n",
    "    for f in files:\n",
    "        if f.endswith(\".pth\"):\n",
    "            print(f\"{indent}  └ {f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b6454be-2802-45f5-b75b-41752267812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, norm=nn.BatchNorm2d, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            norm(out_channels),\n",
    "            activation()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class DeconvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, output_padding=0, norm=nn.BatchNorm2d, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding),\n",
    "            norm(out_channels),\n",
    "            activation()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215589e-d300-474f-9467-302a7c2cf7e9",
   "metadata": {},
   "source": [
    "# VelocityGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b2c1cb8-b2f0-4f65-baf5-122a89a3f722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VelocityGAN(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, base_c=32, norm=nn.BatchNorm2d, activation=nn.ReLU):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.convblock1 = ConvBlock(in_channels, base_c, norm=norm, activation=activation)\n",
    "        self.convblock2_1 = ConvBlock(base_c, base_c*2, stride=2, norm=norm, activation=activation)\n",
    "        self.convblock2_2 = ConvBlock(base_c*2, base_c*2, norm=norm, activation=activation)\n",
    "        self.convblock3_1 = ConvBlock(base_c*2, base_c*4, stride=2, norm=norm, activation=activation)\n",
    "        self.convblock3_2 = ConvBlock(base_c*4, base_c*4, norm=norm, activation=activation)\n",
    "        self.convblock4_1 = ConvBlock(base_c*4, base_c*8, stride=2, norm=norm, activation=activation)\n",
    "        self.convblock4_2 = ConvBlock(base_c*8, base_c*8, norm=norm, activation=activation)\n",
    "        self.convblock5_1 = ConvBlock(base_c*8, base_c*8, stride=2, norm=norm, activation=activation)\n",
    "        self.convblock5_2 = ConvBlock(base_c*8, base_c*8, norm=norm, activation=activation)\n",
    "        self.convblock6_1 = ConvBlock(base_c*8, base_c*8, stride=2, norm=norm, activation=activation)\n",
    "        self.convblock6_2 = ConvBlock(base_c*8, base_c*8, norm=norm, activation=activation)\n",
    "        self.convblock7_1 = ConvBlock(base_c*8, base_c*8, stride=2, norm=norm, activation=activation)\n",
    "        self.convblock7_2 = ConvBlock(base_c*8, base_c*8, norm=norm, activation=activation)\n",
    "        self.convblock8 = ConvBlock(base_c*8, base_c*8, norm=norm, activation=activation)\n",
    "\n",
    "        # Decoder\n",
    "        self.deconv1_1 = DeconvBlock(base_c*8, base_c*8, stride=2, output_padding=1, norm=norm, activation=activation)\n",
    "        self.deconv1_2 = DeconvBlock(base_c*16, base_c*8, norm=norm, activation=activation)\n",
    "        self.deconv2_1 = DeconvBlock(base_c*8, base_c*8, stride=2, output_padding=1, norm=norm, activation=activation)\n",
    "        self.deconv2_2 = DeconvBlock(base_c*16, base_c*8, norm=norm, activation=activation)\n",
    "        self.deconv3_1 = DeconvBlock(base_c*8, base_c*8, stride=2, output_padding=1, norm=norm, activation=activation)\n",
    "        self.deconv3_2 = DeconvBlock(base_c*16, base_c*8, norm=norm, activation=activation)\n",
    "        self.deconv4_1 = DeconvBlock(base_c*8, base_c*8, stride=2, output_padding=1, norm=norm, activation=activation)\n",
    "        self.deconv4_2 = DeconvBlock(base_c*16, base_c*4, norm=norm, activation=activation)\n",
    "        self.deconv5_1 = DeconvBlock(base_c*4, base_c*4, stride=2, output_padding=1, norm=norm, activation=activation)\n",
    "        self.deconv5_2 = DeconvBlock(base_c*8, base_c*2, norm=norm, activation=activation)\n",
    "        self.deconv6 = DeconvBlock(base_c*2, base_c, stride=2, output_padding=1, norm=norm, activation=activation)\n",
    "\n",
    "        self.final = nn.Conv2d(base_c, out_channels, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.convblock1(x)\n",
    "        x2 = self.convblock2_1(x1)\n",
    "        x2 = self.convblock2_2(x2)\n",
    "        x3 = self.convblock3_1(x2)\n",
    "        x3 = self.convblock3_2(x3)\n",
    "        x4 = self.convblock4_1(x3)\n",
    "        x4 = self.convblock4_2(x4)\n",
    "        x5 = self.convblock5_1(x4)\n",
    "        x5 = self.convblock5_2(x5)\n",
    "        x6 = self.convblock6_1(x5)\n",
    "        x6 = self.convblock6_2(x6)\n",
    "        x7 = self.convblock7_1(x6)\n",
    "        x7 = self.convblock7_2(x7)\n",
    "        x8 = self.convblock8(x7)\n",
    "        # Decoder\n",
    "        d1 = self.deconv1_1(x8)\n",
    "        d1 = torch.cat([d1, x7], dim=1)\n",
    "        d1 = self.deconv1_2(d1)\n",
    "        d2 = self.deconv2_1(d1)\n",
    "        d2 = torch.cat([d2, x6], dim=1)\n",
    "        d2 = self.deconv2_2(d2)\n",
    "        d3 = self.deconv3_1(d2)\n",
    "        d3 = torch.cat([d3, x5], dim=1)\n",
    "        d3 = self.deconv3_2(d3)\n",
    "        d4 = self.deconv4_1(d3)\n",
    "        d4 = torch.cat([d4, x4], dim=1)\n",
    "        d4 = self.deconv4_2(d4)\n",
    "        d5 = self.deconv5_1(d4)\n",
    "        d5 = torch.cat([d5, x3], dim=1)\n",
    "        d5 = self.deconv5_2(d5)\n",
    "        d6 = self.deconv6(d5)\n",
    "        d6 = torch.cat([d6, x2], dim=1)\n",
    "        out = self.final(d6)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85f8ab75-c392-43b1-a4db-231f630bbdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 model files to ensemble.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def collect_model_paths(root):\n",
    "    model_files = []\n",
    "    for subdir, _, files in os.walk(root):\n",
    "        for f in files:\n",
    "            if f.endswith('.pth'):\n",
    "                model_files.append(os.path.join(subdir, f))\n",
    "    return sorted(model_files)\n",
    "\n",
    "def build_ensemble(model_paths, device='cuda'):\n",
    "    base_model = VelocityGAN().to(device)\n",
    "    state_dicts = []\n",
    "    for path in model_paths:\n",
    "        sd = torch.load(path, map_location=device, weights_only=False)\n",
    "        # --- state_dictの在りかを自動判定 ---\n",
    "        if 'model' in sd:\n",
    "            # OpenFWI/VelocityGAN公式checkpoint\n",
    "            state_dicts.append(sd['model'])\n",
    "        elif 'state_dict' in sd:\n",
    "            state_dicts.append(sd['state_dict'])\n",
    "        elif 'model_state_dict' in sd:\n",
    "            state_dicts.append(sd['model_state_dict'])\n",
    "        else:\n",
    "            # すでにstate_dictそのもの\n",
    "            state_dicts.append(sd)\n",
    "    # --- 平均化 ---\n",
    "    avg_state = {}\n",
    "    keys = state_dicts[0].keys()\n",
    "    for key in keys:\n",
    "        if isinstance(state_dicts[0][key], torch.Tensor):\n",
    "            stacked = torch.stack([sd[key].float() for sd in state_dicts], dim=0)\n",
    "            avg_state[key] = torch.mean(stacked, dim=0)\n",
    "        else:\n",
    "            avg_state[key] = state_dicts[0][key]\n",
    "    # --- ロード ---\n",
    "    base_model.load_state_dict(avg_state)\n",
    "    return base_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 使用例\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "dir_root = \"../models\"\n",
    "paths = collect_model_paths(dir_root)\n",
    "print(f\"Found {len(paths)} model files to ensemble.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91794c8d-beac-4ff9-ac5a-13b4cfb9fd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found .pth files:\n",
      "../models/CurveFault_A/curvefault_a_l1_480.pth\n",
      "../models/CurveFault_A/curvefault_a_l2_480.pth\n",
      "../models/CurveFault_B/curvefault_b_l1_480.pth\n",
      "../models/CurveFault_B/curvefault_b_l2_480.pth\n",
      "../models/CurveVel_A/curvevel_a_l1_480.pth\n",
      "../models/CurveVel_A/curvevel_a_l2_480.pth\n",
      "../models/CurveVel_B/curvevel_b_l1_480.pth\n",
      "../models/CurveVel_B/curvevel_b_l2_480.pth\n",
      "../models/FlatFault_A/flatfault_a_l1_480.pth\n",
      "../models/FlatFault_A/flatfault_a_l2_480.pth\n",
      "../models/FlatFault_B/flatfault_b_l1_480.pth\n",
      "../models/FlatFault_B/flatfault_b_l2_480.pth\n",
      "../models/FlatVel_A/flatvel_a_l1_480.pth\n",
      "../models/FlatVel_A/flatvel_a_l2_480.pth\n",
      "../models/FlatVel_B/flatvel_b_l1_480.pth\n",
      "../models/FlatVel_B/flatvel_b_l2_480.pth\n",
      "../models/Style_A/style_a_l1_480.pth\n",
      "../models/Style_A/style_a_l2_480.pth\n",
      "../models/Style_B/style_b_l1_480.pth\n",
      "../models/Style_B/style_b_l2_480.pth\n",
      "Total: 20\n"
     ]
    }
   ],
   "source": [
    "dir_root = \"../models\"\n",
    "\n",
    "# ファイル探索で確認\n",
    "import os\n",
    "paths = []\n",
    "for subdir, _, files in os.walk(dir_root):\n",
    "    for f in files:\n",
    "        if f.endswith('.pth'):\n",
    "            paths.append(os.path.join(subdir, f))\n",
    "print(\"Found .pth files:\")\n",
    "for p in paths:\n",
    "    print(p)\n",
    "print(f\"Total: {len(paths)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f7bdc3-0bec-4c38-8f23-f29e449f859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_model = build_ensemble(paths, device)\n",
    "save_path = os.path.join(dir_root, 'velocitygan_ensemble.pth')\n",
    "torch.save(ensemble_model.state_dict(), save_path)\n",
    "print(f\"Ensembled model saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b1c98-bdd6-4966-8197-d09900aad8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
